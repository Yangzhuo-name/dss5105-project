# test_binary_classification.py
"""
äºŒåˆ†ç±»æµ‹è¯•ï¼šèƒ½å›ç­” vs ä¸èƒ½å›ç­”
ç›®æ ‡ï¼šå‡†ç¡®ç‡ â‰¥ 85%
"""

from src.chat import ask
import json
from datetime import datetime

# äºŒåˆ†ç±»æµ‹è¯•ç”¨ä¾‹
TEST_CASES = {
    "CanAnswer": [
        # åˆåŒä¸­æ˜ç¡®æœ‰ç­”æ¡ˆçš„
        "When is my rent due each month?",
        "What is the security deposit amount?",
        "Who pays for electricity and water?",
        "What's the diplomatic clause?",
        "Who is responsible for air conditioning maintenance?",
        "Can I keep pets?",
        "Who pays for repairs under $200?",
        
        # éœ€è¦æ¨ç†ä½†èƒ½ä»åˆåŒæ¨å‡ºçš„
        "Can I install a dishwasher?",
        "What if the aircon breaks during the first week?",
        "Can my parents visit and stay for 2 months?",
    ],
    
    "CannotAnswer": [
        # å®Œå…¨ä¸åœ¨åˆåŒä¸­çš„é—®é¢˜
        "What's the best internet service provider in Singapore?",
        "How do I apply for a work permit?",
        "Where can I buy furniture nearby?",
        "What's the weather like in Singapore?",
        "How do I open a bank account in DBS?",
        "Which primary school is good for my children?",
        "Where is the nearest MRT station?",
    ]
}

def test_binary_classification():
    """æµ‹è¯•äºŒåˆ†ç±»å‡†ç¡®ç‡"""
    print("="*80)
    print("ğŸ§ª BINARY CLASSIFICATION TEST")
    print("="*80)
    print(f"â° Test Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ¯ Target Accuracy: â‰¥85%")
    
    total_questions = sum(len(questions) for questions in TEST_CASES.values())
    print(f"\nğŸ“ Total Questions: {total_questions}")
    print(f"   - CanAnswer: {len(TEST_CASES['CanAnswer'])}")
    print(f"   - CannotAnswer: {len(TEST_CASES['CannotAnswer'])}")
    print("="*80)
    
    all_results = []
    correct_count = 0
    confusion_matrix = {
        "CanAnswer": {"CanAnswer": 0, "CannotAnswer": 0},
        "CannotAnswer": {"CanAnswer": 0, "CannotAnswer": 0}
    }
    
    # æµ‹è¯•æ¯ä¸ªç±»åˆ«
    for expected, questions in TEST_CASES.items():
        print(f"\n{'='*80}")
        print(f"ğŸ” Testing {expected} Questions")
        print("="*80)
        
        for i, question in enumerate(questions, 1):
            print(f"\n[{i}/{len(questions)}] {question}")
            
            try:
                response = ask(question)
                
                # æå–ä¿¡æ¯
                can_answer = response.get('can_answer', False)
                actual = "CanAnswer" if can_answer else "CannotAnswer"
                score = response.get('score', 0.0)
                
                # åˆ¤æ–­æ˜¯å¦æ­£ç¡®
                is_correct = (actual == expected)
                if is_correct:
                    correct_count += 1
                    symbol = "âœ…"
                else:
                    symbol = "âŒ"
                
                # æ›´æ–°æ··æ·†çŸ©é˜µ
                confusion_matrix[expected][actual] += 1
                
                print(f"   {symbol} Expected: {expected} | Got: {actual} | Score: {score:.3f}")
                
                # ä¿å­˜ç»“æœ
                all_results.append({
                    'question': question,
                    'expected': expected,
                    'actual': actual,
                    'score': score,
                    'correct': is_correct
                })
                
            except Exception as e:
                print(f"   âŒ Error: {str(e)}")
                all_results.append({
                    'question': question,
                    'expected': expected,
                    'actual': 'Error',
                    'score': 1.0,
                    'correct': False
                })
    
    # è®¡ç®—å‡†ç¡®ç‡
    accuracy = (correct_count / total_questions) * 100
    
    # æ˜¾ç¤ºæ··æ·†çŸ©é˜µ
    print("\n" + "="*80)
    print("ğŸ“Š CONFUSION MATRIX")
    print("="*80)
    print("\n                    Predicted â†’")
    print("Actual â†“         CanAnswer  CannotAnswer")
    print("-" * 50)
    for expected in ["CanAnswer", "CannotAnswer"]:
        counts = confusion_matrix[expected]
        print(f"{expected:15}  {counts['CanAnswer']:10}  {counts['CannotAnswer']:12}")
    
    # åˆ†ç±»åˆ«å‡†ç¡®ç‡
    print("\n" + "="*80)
    print("ğŸ“ˆ PER-CLASS ACCURACY")
    print("="*80)
    
    for expected in ["CanAnswer", "CannotAnswer"]:
        total_in_class = len(TEST_CASES[expected])
        correct_in_class = confusion_matrix[expected][expected]
        class_accuracy = (correct_in_class / total_in_class) * 100 if total_in_class > 0 else 0
        
        if class_accuracy >= 85:
            marker = "âœ…"
        elif class_accuracy >= 70:
            marker = "âš ï¸ "
        else:
            marker = "âŒ"
        
        print(f"{marker} {expected:15}: {correct_in_class}/{total_in_class} = {class_accuracy:.1f}%")
    
    # æ€»ç»“
    print("\n" + "="*80)
    print("ğŸ¯ OVERALL SUMMARY")
    print("="*80)
    print(f"\nğŸ“ Total Questions: {total_questions}")
    print(f"âœ… Correctly Classified: {correct_count}")
    print(f"âŒ Misclassified: {total_questions - correct_count}")
    print(f"\nğŸ“Š Overall Accuracy: {accuracy:.1f}%")
    
    if accuracy >= 85:
        print(f"\nğŸ‰ TARGET ACHIEVED! (â‰¥85%)")
    else:
        print(f"\nâš ï¸  Below target. Need {85 - accuracy:.1f}% improvement")
    
    # æ˜¾ç¤ºé”™è¯¯æ¡ˆä¾‹
    errors = [r for r in all_results if not r['correct']]
    if errors:
        print("\n" + "="*80)
        print(f"âŒ MISCLASSIFIED CASES ({len(errors)})")
        print("="*80)
        for err in errors:
            print(f"\nQ: {err['question']}")
            print(f"   Expected: {err['expected']} â†’ Got: {err['actual']}")
            print(f"   Score: {err.get('score', 'N/A'):.3f}")
    
    # ä¿å­˜ç»“æœ
    output = {
        'test_time': datetime.now().isoformat(),
        'classification_type': 'binary',
        'total_questions': total_questions,
        'correct': correct_count,
        'accuracy': accuracy,
        'confusion_matrix': confusion_matrix,
        'results': all_results
    }
    
    with open('binary_classification_results.json', 'w', encoding='utf-8') as f:
        json.dump(output, f, indent=2, ensure_ascii=False)
    
    print("\n" + "="*80)
    print(f"ğŸ’¾ Results saved to: binary_classification_results.json")
    print("="*80)
    
    return output


if __name__ == "__main__":
    print("\nğŸš€ Starting binary classification test...\n")
    results = test_binary_classification()
    
    print(f"\nâœ… Test complete!")
    print(f"ğŸ¯ Final Accuracy: {results['accuracy']:.1f}%")